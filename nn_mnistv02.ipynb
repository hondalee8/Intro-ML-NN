{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZu_wx6erFCk"
   },
   "source": [
    "# **Handwritten Digit Recognition(MNIST dataset) -- Intro Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gm2M4e6jrRmA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_RJq7MgrVIa"
   },
   "source": [
    "## **Step 1: import data by dataloader in batch**\n",
    "\n",
    "\n",
    "*   train_data = 60000 images\n",
    "*   test_data = 10000 images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXmqUOyesN8q"
   },
   "outputs": [],
   "source": [
    "#import dataset\n",
    "transform = transforms.ToTensor()\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "print(\"number of image in train_data:{} | no. of image in test_data: {}\\n\".format(len(train_data), len(test_data)))\n",
    "\n",
    "#mini batch(each batch contain 60 images)\n",
    "batch_size = 60 #each batch contain 60 images\n",
    "trainset = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8v_vXnIscIa"
   },
   "source": [
    "\n",
    "## **Step 2: Knowning the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf4vylPQskgZ"
   },
   "source": [
    "### How many image in each training batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZsSLtEEsr-I"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainset)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QFBLTkbsx2A"
   },
   "source": [
    "Finding:\n",
    "\n",
    "There are 60 images in each batch, each image dimension is 28*28 pixels\n",
    "\n",
    "There are 60 labels in each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qmA6aIttUY0"
   },
   "source": [
    "### Show the image in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GC7rxa5XtfEb"
   },
   "outputs": [],
   "source": [
    "plt.imshow(images[3].numpy().squeeze(), cmap='gray_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6FqLHb5Bzli"
   },
   "outputs": [],
   "source": [
    " torch.set_printoptions(linewidth=300)\n",
    " print(images[3])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4hEPAPCtjXY"
   },
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "num_of_images = 20\n",
    "for index in range(1, num_of_images + 1):\n",
    "    plt.subplot(5, 10, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWGs0XaZtnuR"
   },
   "source": [
    "## **Step 3: Build the Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLpAgSu_tslJ"
   },
   "source": [
    "### Define class for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZMCNp44tyyB"
   },
   "outputs": [],
   "source": [
    "#define modle class\n",
    "class MLP_Net (nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(28*28, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784) #flatten the data from(n,1,28,28) -> (n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        x = self.l5(x)  \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jR2zKclSt5QI"
   },
   "source": [
    "### Define loss and optimizer function, by using pytorch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOy8mqZ3t-A2"
   },
   "outputs": [],
   "source": [
    "model = MLP_Net()\n",
    "\n",
    "# define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBRkVqbfuByb"
   },
   "source": [
    "### Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ovw4-ml3uK2J"
   },
   "outputs": [],
   "source": [
    "def training_loop(n_epoch):\n",
    "    for epoch in range(n_epoch):\n",
    "        for batch_idx, (data, target) in enumerate(trainset):\n",
    "            #print(data.shape)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Train Epoch: {} | Batch_idx: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                    epoch, batch_idx,  batch_idx * len(data), len(train_data),\n",
    "                    100. * batch_idx*len(data) / len(train_data), loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcIVdP1RuN-0"
   },
   "source": [
    "## **Step 4: Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TBHNA8jubry"
   },
   "outputs": [],
   "source": [
    "n_epoch =3\n",
    "training_loop(n_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJkVlZLTuWhl"
   },
   "source": [
    "## **Step 5: Predict value by using the trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mvc4-VrukGz"
   },
   "outputs": [],
   "source": [
    "i=1003\n",
    "\n",
    "#image data\n",
    "image = test_data[i][0]\n",
    "\n",
    "#do prediction:\n",
    "p = model(image.view(-1, 28*28))\n",
    "\n",
    "print('\\npredict result is :\\n', p)\n",
    "print ('\\npredict value is : ', torch.argmax(p))\n",
    "print('\\nThe  label of of the image is: {} \\n'.format(test_data[i][1]))\n",
    "plt.imshow(image.numpy()[0], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1RJGtREur2r"
   },
   "source": [
    "## **Step 6: Calculate the Model Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AaqW_EouzxP"
   },
   "outputs": [],
   "source": [
    "def cal_accuracy():\n",
    "  total_count=0\n",
    "  correct_count =0\n",
    "  for image,label in test_data:\n",
    "    p = model(image.view(-1, 28*28))\n",
    "    pred_value = torch.argmax(p)\n",
    "    if (pred_value == label):\n",
    "      correct_count +=1 \n",
    "    total_count+=1\n",
    "  print(\"Total= {0}, Correct = {1}\".format(total_count, correct_count))\n",
    "  print(\"Accuracy ={0}\".format(correct_count/total_count))\n",
    "\n",
    "cal_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSMRm-8ZOuCW"
   },
   "source": [
    "## **Appendix 1: Whole code to the Model building and training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzXKWJ6IRWqk"
   },
   "source": [
    "**Appendix 1.1 Building model and training model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1625569821069,
     "user": {
      "displayName": "honda lee",
      "photoUrl": "",
      "userId": "13942233707715688949"
     },
     "user_tz": -480
    },
    "id": "vRydx0UZOz3F",
    "outputId": "9928cf2b-d8d0-4059-a80b-a95711b9a4c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of image in train_data:60000 | no. of image in test_data: 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#import dataset\n",
    "transform = transforms.ToTensor()\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "print(\"number of image in train_data:{} | no. of image in test_data: {}\\n\".format(len(train_data), len(test_data)))\n",
    "\n",
    "#mini batch(each batch contain 60 images)\n",
    "batch_size = 60 #each batch contain 60 images\n",
    "trainset = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#define modle class\n",
    "class MLP_Net (nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(28*28, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784) #flatten the data from(n,1,28,28) -> (n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        x = self.l5(x)  \n",
    "        return(x)\n",
    "\n",
    "#make an instance of the model\n",
    "model = MLP_Net()\n",
    "\n",
    "# define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "def training_loop(n_epoch):\n",
    "    for epoch in range(n_epoch):\n",
    "        for batch_idx, (data, target) in enumerate(trainset):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "              print('Train Epoch: {} | Batch_idx: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                    epoch, batch_idx,  batch_idx * len(data), len(train_data),\n",
    "                    100. * batch_idx*len(data) / len(train_data), loss.item()))\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SsTuRNIetC1"
   },
   "source": [
    "**Appendix 1.1 Model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35596,
     "status": "ok",
     "timestamp": 1625569652044,
     "user": {
      "displayName": "honda lee",
      "photoUrl": "",
      "userId": "13942233707715688949"
     },
     "user_tz": -480
    },
    "id": "4DhdpUDZezai",
    "outputId": "e0f1439a-af08-4fb6-b914-5d613bb5ba05",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 | Batch_idx: 0 | Batch Status: 0/60000 (0%) | Loss: 2.305997\n",
      "Train Epoch: 0 | Batch_idx: 100 | Batch Status: 6000/60000 (10%) | Loss: 2.295703\n",
      "Train Epoch: 0 | Batch_idx: 200 | Batch Status: 12000/60000 (20%) | Loss: 2.300183\n",
      "Train Epoch: 0 | Batch_idx: 300 | Batch Status: 18000/60000 (30%) | Loss: 2.284393\n",
      "Train Epoch: 0 | Batch_idx: 400 | Batch Status: 24000/60000 (40%) | Loss: 2.293286\n",
      "Train Epoch: 0 | Batch_idx: 500 | Batch Status: 30000/60000 (50%) | Loss: 2.278712\n",
      "Train Epoch: 0 | Batch_idx: 600 | Batch Status: 36000/60000 (60%) | Loss: 2.255322\n",
      "Train Epoch: 0 | Batch_idx: 700 | Batch Status: 42000/60000 (70%) | Loss: 2.242053\n",
      "Train Epoch: 0 | Batch_idx: 800 | Batch Status: 48000/60000 (80%) | Loss: 2.129200\n",
      "Train Epoch: 0 | Batch_idx: 900 | Batch Status: 54000/60000 (90%) | Loss: 1.787087\n",
      "Train Epoch: 1 | Batch_idx: 0 | Batch Status: 0/60000 (0%) | Loss: 1.541230\n",
      "Train Epoch: 1 | Batch_idx: 100 | Batch Status: 6000/60000 (10%) | Loss: 1.015083\n",
      "Train Epoch: 1 | Batch_idx: 200 | Batch Status: 12000/60000 (20%) | Loss: 0.956727\n",
      "Train Epoch: 1 | Batch_idx: 300 | Batch Status: 18000/60000 (30%) | Loss: 0.704538\n",
      "Train Epoch: 1 | Batch_idx: 400 | Batch Status: 24000/60000 (40%) | Loss: 0.785050\n",
      "Train Epoch: 1 | Batch_idx: 500 | Batch Status: 30000/60000 (50%) | Loss: 0.762102\n",
      "Train Epoch: 1 | Batch_idx: 600 | Batch Status: 36000/60000 (60%) | Loss: 0.456707\n",
      "Train Epoch: 1 | Batch_idx: 700 | Batch Status: 42000/60000 (70%) | Loss: 0.564937\n",
      "Train Epoch: 1 | Batch_idx: 800 | Batch Status: 48000/60000 (80%) | Loss: 0.642047\n",
      "Train Epoch: 1 | Batch_idx: 900 | Batch Status: 54000/60000 (90%) | Loss: 0.545138\n",
      "Train Epoch: 2 | Batch_idx: 0 | Batch Status: 0/60000 (0%) | Loss: 0.452233\n",
      "Train Epoch: 2 | Batch_idx: 100 | Batch Status: 6000/60000 (10%) | Loss: 0.308888\n",
      "Train Epoch: 2 | Batch_idx: 200 | Batch Status: 12000/60000 (20%) | Loss: 0.407194\n",
      "Train Epoch: 2 | Batch_idx: 300 | Batch Status: 18000/60000 (30%) | Loss: 0.283979\n",
      "Train Epoch: 2 | Batch_idx: 400 | Batch Status: 24000/60000 (40%) | Loss: 0.466044\n",
      "Train Epoch: 2 | Batch_idx: 500 | Batch Status: 30000/60000 (50%) | Loss: 0.281021\n",
      "Train Epoch: 2 | Batch_idx: 600 | Batch Status: 36000/60000 (60%) | Loss: 0.151181\n",
      "Train Epoch: 2 | Batch_idx: 700 | Batch Status: 42000/60000 (70%) | Loss: 0.656954\n",
      "Train Epoch: 2 | Batch_idx: 800 | Batch Status: 48000/60000 (80%) | Loss: 0.284575\n",
      "Train Epoch: 2 | Batch_idx: 900 | Batch Status: 54000/60000 (90%) | Loss: 0.421220\n"
     ]
    }
   ],
   "source": [
    "#Model training\n",
    "n_epoch =3\n",
    "training_loop(n_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1o5CO6MARhBc"
   },
   "source": [
    "**Appendix 1.2 Prediction base on trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1625569826661,
     "user": {
      "displayName": "honda lee",
      "photoUrl": "",
      "userId": "13942233707715688949"
     },
     "user_tz": -480
    },
    "id": "q8_rq84uRD11",
    "outputId": "812c0b20-3d80-4a0c-d203-2001e234bab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "predict result is :\n",
      " tensor([[-0.0615, -0.0463, -0.0493,  0.0115,  0.0244, -0.0397,  0.0667,  0.0695,\n",
      "          0.0151,  0.0681]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "predict value is :  tensor(7)\n",
      "\n",
      "The  label of of the image is: 5 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f085645b110>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=1003\n",
    "image = test_data[i][0]\n",
    "\n",
    "#do prediction:\n",
    "p = model(image.view(-1, 28*28))\n",
    "print('\\npredict result is :\\n', p)\n",
    "print ('\\npredict value is : ', torch.argmax(p))\n",
    "print('\\nThe  label of of the image is: {} \\n'.format(test_data[i][1]))\n",
    "plt.imshow(image.numpy()[0], cmap='gray')                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGGdJnMeu5-7"
   },
   "source": [
    "## **Appendix 2: Save Model to disk and use saved_model to predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwbilzS9u9bt"
   },
   "outputs": [],
   "source": [
    "#save trained module to disk\n",
    "torch.save(model, 'data/my_mnist_model.pt') \n",
    "\n",
    "#load trained module\n",
    "load_model = torch.load('data/my_mnist_model.pt')\n",
    "load_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XaIjcxu_vFoO"
   },
   "outputs": [],
   "source": [
    "#do prediction\n",
    "with torch.no_grad():\n",
    "  i=12\n",
    "  image = test_data[i][0]\n",
    "  p = load_model(image.view(-1, 28*28))\n",
    "  #print('\\npredict result is :\\n', p)\n",
    "  print (\"predict value ={0} , label of the image = {1}\".format(torch.argmax(p), test_data[i][1]))\n",
    "  plt.imshow(image.numpy()[0], cmap='gray') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skZF4Pxrvc_W"
   },
   "source": [
    "ref: https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO6rbGBoRaw50Yd/T/8+H2F",
   "collapsed_sections": [],
   "name": "nn_mnistv02.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
